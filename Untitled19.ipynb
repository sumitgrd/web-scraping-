{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dade8800",
   "metadata": {},
   "source": [
    "# WEB SCRAPING-ASSIGNMENT3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfff186",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in. The\n",
    "product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for\n",
    "guitars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a6af59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.17.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4862da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter product here--->guiter\n"
     ]
    }
   ],
   "source": [
    "inputU = input('please enter product here--->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ddc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_xpath('//*[@id=\"twotabsearchtextbox\"]')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(inputU)       # Inputing keyword to search \n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"nav-search-submit-button\"]')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "productName=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9847ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Product_Name \n",
    "PName=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in PName:\n",
    "    if i.text is None :\n",
    "        productName.append(\"--\") \n",
    "    else:\n",
    "        productName.append(i.text)\n",
    "print(len(productName),productName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2274cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d97a601",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your search\n",
    "results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then\n",
    "scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc54a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = 0\n",
    "end_page = 3\n",
    "urls = []\n",
    "for page in range(start_page,end_page+1):\n",
    "    try:\n",
    "        page_urls = driver.find_elements_by_xpath('//a[@class=\"a-link-normal s-no-outline\"]')\n",
    "        \n",
    "        # appending all the urls on current page to urls list\n",
    "        for url in page_urls:\n",
    "            url = url.get_attribute('href')     # Scraping the url from webelement\n",
    "            if url[0:4]=='http':                # Checking if the scraped data is a valid url or not\n",
    "                urls.append(url)                # Appending the url to urls list\n",
    "        print(\"Product urls of page {} has been scraped.\".format(page+1))\n",
    "        \n",
    "        # Moving to next page\n",
    "        nxt_button = driver.find_element_by_xpath('//li[@class=\"a-last\"]/a')      # Locating the next_button which is active\n",
    "        if nxt_button.text == 'Next→':                                            # Checking if the button located is next button\n",
    "            nxt_button.click()                                                    # Clicking the next button\n",
    "            time.sleep(5)                                                         # time delay of 5 seconds\n",
    "        # If the current active button is not next button, the we will check if the next button is inactive or not    \n",
    "        elif driver.find_element_by_xpath('//li[@class=\"a-disabled a-last\"]/a').text == 'Next→':    \n",
    "            print(\"No new pages exist. Breaking the loop\")  # Printing message and breakinf loop if we have reached the last page\n",
    "            break\n",
    "            \n",
    "    except StaleElementReferenceException as e:             # Handling StaleElement Exception   \n",
    "        print(\"Stale Exception\")\n",
    "        next_page = nxt_button.get_attribute('href')        # Extracting the url of next page\n",
    "        driver.get(next_page)                               # ReLoading the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_dict = {}\n",
    "prod_dict['Brand']=[]\n",
    "prod_dict['Name']=[]\n",
    "prod_dict['Rating']=[]\n",
    "prod_dict['No. of ratings']=[]\n",
    "prod_dict['Price']=[]\n",
    "prod_dict['Return/Exchange']=[]\n",
    "prod_dict['Expected Delivery']=[] \n",
    "prod_dict['Availability']=[]\n",
    "prod_dict['Other Details']=[]\n",
    "prod_dict['URL']=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls[:4]:\n",
    "    driver.get(url)                                                        # Loading the webpage by url\n",
    "    print(\"Scraping URL = \", url)\n",
    "    #time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//a[@id=\"bylineInfo\"]')      # Extracting Brand from xpath\n",
    "        prod_dict['Brand'].append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Brand'].append('-')\n",
    "    \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//h1[@id=\"title\"]/span')      # Extracting Name from xpath\n",
    "        prod_dict['Name'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Name'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath('//span[@id=\"acrPopover\"]')  # Extracting Ratings from xpath\n",
    "        prod_dict['Rating'].append(rating.get_attribute(\"title\"))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Rating'].append('-')\n",
    "    \n",
    "    try:\n",
    "        n_rating = driver.find_element_by_xpath('//a[@id=\"acrCustomerReviewLink\"]/span')     # Extracting no. of Ratings from xpath\n",
    "        prod_dict['No. of ratings'].append(n_rating.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['No. of ratings'].append('-')\n",
    "    \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//span[@id=\"priceblock_ourprice\"]')            # Extracting Price from xpath\n",
    "        prod_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Price'].append('-')\n",
    "    try:                                                                                     # Extracting Return/Exchange policy from xpath\n",
    "        ret = driver.find_element_by_xpath('//div[@data-name=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        prod_dict['Return/Exchange'].append(ret.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Return/Exchange'].append('-')\n",
    "    try:\n",
    "        delivry = driver.find_element_by_xpath('//div[@id=\"ddmDeliveryMessage\"]/b')         # Extracting Expected Delivery from xpath\n",
    "        prod_dict['Expected Delivery'].append(delivry.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Expected Delivery'].append('-')\n",
    "    \n",
    "    try:\n",
    "        avl = driver.find_element_by_xpath('//div[@id=\"availability\"]/span')                # Extracting Availability from xpath\n",
    "        prod_dict['Availability'].append(avl.text)\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Availability'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Other Details from xpath\n",
    "        dtls = driver.find_element_by_xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]')\n",
    "        prod_dict['Other Details'].append('  ||  '.join(dtls.text.split('\\n')))\n",
    "    except NoSuchElementException:\n",
    "        prod_dict['Other Details'].append('-')\n",
    "    \n",
    "    prod_dict['URL'].append(url)                                                            # Saving url\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d051c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_df = pd.DataFrame.from_dict(prod_dict)\n",
    "prod_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ade19f",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://images.google.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cb1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/div/div[2]/input')    # Finding the search bar using it's xpath\n",
    "search_bar.send_keys(\"fruits\")       # Inputing \"banana\" keyword to search rock images\n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/button')    # Finding the xpath of search button\n",
    "search_button.click()        # Clicking the search button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"start scrolling to generate more images on the page...\")\n",
    "# 500 time we scroll down by 10000 in order to generate more images on the website\n",
    "for _ in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ad29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    " images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d1e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "len(img_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4c0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(\"H:/Flip ROBO/banana/img\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679e59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7ceae56",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com\n",
    "and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand\n",
    "Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350111d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url4=\"https://www.flipkart.com/search?q=smartphone&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "driver.get(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713d9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_Name=[]\n",
    "Colour=[]\n",
    "Storage_RAM_ROM=[]\n",
    "P_F_Camera=[]\n",
    "Display_size_Resolution=[]\n",
    "ProcessorAndCores=[]\n",
    "Battery=[]\n",
    "Price=[]\n",
    "Product_URL=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38988bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Brand_Name \n",
    "BName=driver.find_elements_by_xpath(\"//div[@class='_4rR01T']\")\n",
    "for i in BName:\n",
    "    if i.text is None :\n",
    "        Brand_Name.append(\"--\") \n",
    "    else:\n",
    "        Brand_Name.append(i.text)\n",
    "print(len(Brand_Name),Brand_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6672c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "24 ['POCO C3 (Arctic Blue, 64 GB)', 'POCO X3 (Cobalt Blue, 128 GB)', 'POCO C3 (Lime Green, 64 GB)', 'POCO X3 (Shadow Gray, 128 GB)', 'POCO M2 (Slate Blue, 64 GB)', 'Realme Narzo 20 (Glory Silver, 64 GB)', 'POCO C3 (Matte Black, 64 GB)', 'Realme Narzo 20A (Victory Blue, 64 GB)', 'Realme Narzo 20A (Glory Silver, 64 GB)', 'Realme Narzo 20 (Victory Blue, 128 GB)', 'Realme Narzo 20 (Glory Silver, 128 GB)', 'POCO M2 Pro (Green and Greener, 64 GB)', 'Realme 7i (Fusion Blue, 64 GB)', 'POCO M2 Pro (Green and Greener, 64 GB)', 'POCO M2 Pro (Out of the Blue, 64 GB)', 'POCO M2 (Pitch Black, 128 GB)', 'POCO M2 Pro (Two Shades of Black, 64 GB)', 'POCO M2 Pro (Two Shades of Black, 64 GB)', 'Redmi 9i (Nature Green, 64 GB)', 'Infinix Smart 4 Plus (Ocean Wave, 32 GB)', 'POCO X3 (Cobalt Blue, 64 GB)', 'Infinix Smart 4 Plus (Midnight Black, 32 GB)', 'POCO M2 Pro (Out of the Blue, 64 GB)', 'Redmi 9i (Sea Blue, 64 GB)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f300609",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Storage_RAM_ROM \n",
    "ram=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[1]\")\n",
    "for i in ram:\n",
    "    if i.text is None :\n",
    "        Storage_RAM_ROM.append(\"--\") \n",
    "    else:\n",
    "        Storage_RAM_ROM.append(i.text)\n",
    "print(len(Storage_RAM_ROM),Storage_RAM_ROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the P_F_Camera \n",
    "PC=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[3]\")\n",
    "for i in PC:\n",
    "    if i.text is None :\n",
    "        P_F_Camera.append(\"--\") \n",
    "    else:\n",
    "        P_F_Camera.append(i.text)\n",
    "print(len(P_F_Camera),P_F_Camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Display_size_Resolution \n",
    "DS=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[2]\")\n",
    "for i in DS:\n",
    "    if i.text is None :\n",
    "        Display_size_Resolution.append(\"--\") \n",
    "    else:\n",
    "        Display_size_Resolution.append(i.text)\n",
    "print(len(Display_size_Resolution),Display_size_Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the ProcessorAndCores \n",
    "P=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[5]\")\n",
    "for i in P:\n",
    "    if i.text is None :\n",
    "        ProcessorAndCores.append(\"--\") \n",
    "    else:\n",
    "        ProcessorAndCores.append(i.text)\n",
    "print(len(ProcessorAndCores),ProcessorAndCores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ece2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Battery \n",
    "B=driver.find_elements_by_xpath(\"//ul[@class='_1xgFaf']//li[4]\")\n",
    "for i in B:\n",
    "    if i.text is None :\n",
    "        Battery.append(\"--\") \n",
    "    else:\n",
    "        Battery.append(i.text)\n",
    "print(len(Battery),Battery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fdb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping the Price \n",
    "price=driver.find_elements_by_xpath(\"//div[@class='_30jeq3 _1_WHN1']\")\n",
    "for i in price:\n",
    "    if i.text is None :\n",
    "        Price.append(\"--\") \n",
    "    else:\n",
    "        Price.append(i.text)\n",
    "print(len(Price),Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2e688",
   "metadata": {},
   "outputs": [],
   "source": [
    "FlipKart=pd.DataFrame([])\n",
    "FlipKart['Brand_Name']=Brand_Name\n",
    "FlipKart['Storage_RAM_ROM']=Storage_RAM_ROM\n",
    "FlipKart['Amount P_F_Camera']=P_F_Camera\n",
    "FlipKart['Display_size_Resolution']=Display_size_Resolution\n",
    "FlipKart['ProcessorAndCores']=ProcessorAndCores\n",
    "FlipKart['Battery']=Battery\n",
    "FlipKart['Price']=Price\n",
    "\n",
    "FlipKart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f050b",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b6a2fed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#opening google maps\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.google.co.in/maps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      5\u001b[0m city \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnter City Name : \u001b[39m\u001b[38;5;124m'\u001b[39m)                                         \u001b[38;5;66;03m# Enter city to be searched\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    " #opening google maps\n",
    "driver.get(\"https://www.google.co.in/maps\")\n",
    "time.sleep(3)\n",
    "\n",
    "city = input('Enter City Name : ')                                         # Enter city to be searched\n",
    "search = driver.find_element_by_id(\"searchboxinput\")                       # locating search bar\n",
    "search.clear()                                                             # clearing search bar\n",
    "time.sleep(2)\n",
    "search.send_keys(city)                                                     # entering values in search bar\n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")               # locating search button\n",
    "button.click()                                                             # clicking search button\n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e2da61",
   "metadata": {},
   "source": [
    "6. Write a program to scrap all the available details of best gaming laptops from digit.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d9f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86495ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.digit.in/top-products/best-gaming-laptops-40.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea71ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13fc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brands=[]\n",
    "Products_Description=[]\n",
    "Specification=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff09bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "br=driver.find_elements_by_xpath(\"//div[@class='TopNumbeHeading active sticky-footer']\")\n",
    "len(br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88611fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in br:\n",
    "   \n",
    "    Brands.append(str(i.text).replace(\"\\n\",\"\"))\n",
    "Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caadedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "['1.MSI GT76 TITAN DT 9SG',\n",
    " '2.ALIENWARE 17 AREA-51M',\n",
    " '3.HP OMEN 15 2020',\n",
    " '4.ASUS ZEPHYRUS G14',\n",
    " '5.LENOVO LEGION Y540',\n",
    " '6.ASUS ROG ZEPHYRUS G GA502',\n",
    " '7.ASUS ROG ZEPHYRUS S GX531',\n",
    " '8.MSI GT83VR 7RE TITAN SLI',\n",
    " '9.ASUS ROG ZEPHYRUS DUO 15',\n",
    " '10.DELL G3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=driver.find_elements_by_xpath(\"//div[@class='Specs-Wrap']\")\n",
    "len(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f63d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6a95a82",
   "metadata": {},
   "source": [
    "7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped:\n",
    "“Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d39c3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad8c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf35b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Opening the amazon page an automated chrome browser \n",
    "driver.get(\"https://www.amazon.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f69537",
   "metadata": {},
   "source": [
    "8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c1971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c56269",
   "metadata": {},
   "outputs": [],
   "source": [
    "Opening the amazon page an automated chrome browser \n",
    "driver.get(\"https://www.amazon.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1626c0",
   "metadata": {},
   "source": [
    "9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall\n",
    "reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd \n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0aaaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23af46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Opening the hostel world page an automated chrome browser \n",
    "driver.get(\"https://www.hostelworld.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c062449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
